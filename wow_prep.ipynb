{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fd9252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: datasets in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: filelock in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: six in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec4d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (0.1.99)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ddefc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yake in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (0.4.8)\n",
      "Requirement already satisfied: tabulate in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from yake) (0.8.10)\n",
      "Requirement already satisfied: click>=6.0 in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from yake) (8.0.4)\n",
      "Requirement already satisfied: numpy in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from yake) (1.24.3)\n",
      "Requirement already satisfied: segtok in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from yake) (1.5.11)\n",
      "Requirement already satisfied: networkx in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from yake) (2.8.4)\n",
      "Requirement already satisfied: jellyfish in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: regex in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (from segtok->yake) (2022.7.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a148f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bm25\n",
      "  Downloading BM25-1.0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: bm25\n",
      "  Building wheel for bm25 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bm25: filename=BM25-1.0.0-py3-none-any.whl size=1741 sha256=ae46d4ea57b8bdb4e635e92cd434faaefb00bac168931dd35dee14c404a6e15d\n",
      "  Stored in directory: /raid/kavin-intern-maunendra/.cache/pip/wheels/00/0c/3c/eac477a276d6eebe52cb68ef5140c49bed58e5f418ce262301\n",
      "Successfully built bm25\n",
      "Installing collected packages: bm25\n",
      "Successfully installed bm25-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece747a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /raid/kavin-intern-maunendra/anaconda3/lib/python3.11/site-packages (0.0.post7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ef47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d16a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184164/184164 [00:14<00:00, 12695.72it/s]\n",
      "18430it [00:11, 1588.88it/s]\n",
      "981it [00:00, 1124.27it/s]\n",
      "965it [00:01, 732.99it/s]\n",
      "Saving the dataset (2/2 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74092/74092 [00:00<00:00, 106624.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3939/3939 [00:00<00:00, 86314.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3865/3865 [00:00<00:00, 83275.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "from random import shuffle, randint\n",
    "# from tqdm import tqdm\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "\n",
    "### Loading Datasets\n",
    "\n",
    "with open(\"wizard_of_wiki/train.json\", \"r\") as f:\n",
    "    wow_data_train = json.load(f)\n",
    "\n",
    "with open(\"wizard_of_wiki/data.json\", \"r\") as f:\n",
    "    wow_data = json.load(f)\n",
    "\n",
    "with open(\"wizard_of_wiki/test_random_split.json\", \"r\") as f:\n",
    "    wow_data_test = json.load(f)\n",
    "\n",
    "with open(\"wizard_of_wiki/valid_random_split.json\", \"r\") as f:\n",
    "    wow_data_valid = json.load(f)\n",
    "\n",
    "### Knowledge Corpus\n",
    "topic2passage = {}\n",
    "passages = []\n",
    "topics = []\n",
    "\n",
    "for i, conv in enumerate(wow_data):\n",
    "    try:\n",
    "        if topic2passage[conv[\"chosen_topic\"]] != conv[\"chosen_topic_passage\"]:\n",
    "            if len(conv[\"chosen_topic_passage\"]) > len(topic2passage[conv[\"chosen_topic\"]]):\n",
    "                topic2passage[conv[\"chosen_topic\"]] = conv[\"chosen_topic_passage\"]\n",
    "\n",
    "    except KeyError:\n",
    "        topic2passage[conv[\"chosen_topic\"]] = conv[\"chosen_topic_passage\"]\n",
    "        topics.append(conv[\"chosen_topic\"])\n",
    "        passages.append(conv[\"chosen_topic_passage\"])\n",
    "\n",
    "    for j, dial in enumerate(conv[\"dialog\"]):\n",
    "        rp = {k: v for pas in dial[\"retrieved_passages\"] for k, v in pas.items()}\n",
    "        for t, pk in rp.items():\n",
    "            try:\n",
    "                if topic2passage[t] != pk:\n",
    "                    if len(pk) > len(topic2passage[t]):\n",
    "                        topic2passage[t] = pk\n",
    "            except KeyError:\n",
    "                topic2passage[t] = pk\n",
    "                topics.append(t)\n",
    "                passages.append(t)\n",
    "\n",
    "\n",
    "for i, conv in enumerate(wow_data_train):\n",
    "    try:\n",
    "        if topic2passage[conv[\"chosen_topic\"]] != conv[\"chosen_topic_passage\"]:\n",
    "            if len(conv[\"chosen_topic_passage\"]) > len(topic2passage[conv[\"chosen_topic\"]]):\n",
    "                topic2passage[conv[\"chosen_topic\"]] = conv[\"chosen_topic_passage\"]\n",
    "\n",
    "    except KeyError:\n",
    "        topic2passage[conv[\"chosen_topic\"]] = conv[\"chosen_topic_passage\"]\n",
    "        topics.append(conv[\"chosen_topic\"])\n",
    "        passages.append(conv[\"chosen_topic_passage\"])\n",
    "\n",
    "    for j, dial in enumerate(conv[\"dialog\"]):\n",
    "        rp = {k: v for pas in dial[\"retrieved_passages\"] for k, v in pas.items()}\n",
    "        \n",
    "        for t, pk in rp.items():\n",
    "            try:\n",
    "                if topic2passage[t] != pk:\n",
    "                    if len(pk) > len(topic2passage[t]):\n",
    "                        topic2passage[t] = pk\n",
    "            except KeyError:\n",
    "                topic2passage[t] = pk\n",
    "                topics.append(t)\n",
    "                passages.append(t)\n",
    "topic2passage[\"no_passages_used\"] = []\n",
    "topic2id = {}\n",
    "id2topic = {}\n",
    "for i, key in enumerate(topic2passage.keys()):\n",
    "    topic2id[key] = i\n",
    "    id2topic[i] = key\n",
    "\n",
    "### BM25\n",
    "def bm25_tokenizer(text):\n",
    "    text = \" \".join(text)\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "all_para = topic2passage.values()\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(all_para):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "\n",
    "def search(query, n):\n",
    "\n",
    "    # print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -n)[-n:]\n",
    "\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    bm25_idx = [b[\"corpus_id\"] for b in bm25_hits]\n",
    "\n",
    "    shuffle(bm25_idx)\n",
    "    out = [id2topic[i] for i in bm25_idx]\n",
    "\n",
    "\n",
    "    # print(len(bm25_hits))\n",
    "    return out\n",
    "\n",
    "### Dataset Pre-Processing\n",
    "index = 0\n",
    "g7 = 0\n",
    "l7 = 0\n",
    "f = []\n",
    "f2 = []\n",
    "not_found = []\n",
    "full_dat_train = {\n",
    "    \"gold_pass\": [],\n",
    "    \"all_pass\": [],\n",
    "    \"gold_sen\": [],\n",
    "    \"all_sen\": [],\n",
    "    \"last_ut\": [],\n",
    "    \"context\": [],\n",
    "    \"response\": [],\n",
    "    \"context_eou\": [],\n",
    "    \"all_topic\": [],\n",
    "}\n",
    "for i, conv in tqdm(enumerate(wow_data_train)):\n",
    "\n",
    "    input = \"\"\n",
    "    last_ut = \"\"\n",
    "    pseudo = \"\"\n",
    "\n",
    "    for j, dial in enumerate(conv[\"dialog\"]):\n",
    "        if \"Wizard\" in dial[\"speaker\"] and j > 0:\n",
    "            # res_key = custom_kw_extractor_1.extract_keywords(dial[\"text\"])\n",
    "            # rk, res_key = key_exrt(res_key, dial[\"text\"])\n",
    "            # ut_key = custom_kw_extractor_1.extract_keywords(last_ut)\n",
    "            # uk, ut_key = key_exrt(ut_key, last_ut)\n",
    "\n",
    "            # masked_res = masker(dial[\"text\"], rk, pseudo)\n",
    "\n",
    "            # pr_in = f\"ut: {ut_key} res: {res_key}\"\n",
    "            # sr_in = f\"ut: {ut_key} res: {res_key}\"\n",
    "            # mpr_in = f\"ut: {last_ut} res: {masked_res}\"\n",
    "            # msr_in = f\"ut: {last_ut} res: {masked_res}\"\n",
    "            full_dat_train[\"context\"].append(input + dial[\"speaker\"][2:] + \": \")\n",
    "            full_dat_train[\"last_ut\"].append(last_ut)\n",
    "\n",
    "            response = dial[\"text\"]\n",
    "\n",
    "            knowledge = [list(k.keys())[0] for k in dial[\"retrieved_passages\"]]\n",
    "\n",
    "            if len(knowledge) < 7:\n",
    "                knowledge = search(response, 7)\n",
    "                l7 += 1\n",
    "            elif len(knowledge) > 7:\n",
    "                knowledge = knowledge[:7]\n",
    "                g7 += 1\n",
    "\n",
    "            try:\n",
    "                gold_pass = list(dial[\"checked_passage\"].values())[0]\n",
    "            except IndexError:\n",
    "                gold_pass = \"no_passages_used\"\n",
    "\n",
    "            try:\n",
    "                gold_sen = list(dial[\"checked_sentence\"].values())[0]\n",
    "            except IndexError:\n",
    "                gold_sen = \"no_passages_used\"\n",
    "\n",
    "            if gold_pass == \"no_passages_used\" and gold_sen != \"no_passages_used\":\n",
    "                gold_pass = \" \".join(list(dial[\"checked_sentence\"].keys())[0].split(\"_\")[1:-1])\n",
    "\n",
    "            try:\n",
    "                assert type(topic2passage[gold_pass]) == list\n",
    "            except:\n",
    "                gold_pass = \" \".join(list(dial[\"checked_sentence\"].keys())[0].split(\"_\")[1:-1])\n",
    "                try:\n",
    "                    assert type(topic2passage[gold_pass]) == list\n",
    "                except:\n",
    "                    gold_pass = \"no_passages_used\"\n",
    "                    gold_sen = \"no_passages_used\"\n",
    "                    f2.append((i, j))\n",
    "\n",
    "            if gold_pass not in knowledge and gold_pass != \"no_passages_used\":\n",
    "                idx = randint(0, 6)\n",
    "                knowledge[idx] = gold_pass\n",
    "\n",
    "            all_sen = topic2passage[gold_pass]\n",
    "\n",
    "\n",
    "            if gold_pass == \"no_passages_used\":\n",
    "                gid = float(\"-inf\")\n",
    "                sid = float(\"-inf\")\n",
    "            else:\n",
    "                gid = knowledge.index(gold_pass)\n",
    "                try:\n",
    "                    sid = topic2passage[gold_pass].index(gold_sen)\n",
    "                except:\n",
    "                    sid = len(all_sen)\n",
    "                    all_sen += [gold_sen]\n",
    "                    f.append((i, j))\n",
    "\n",
    "            all_pass = [\" \".join(topic2passage[t]) for t in knowledge]\n",
    "            if gid != float(\"-inf\"):\n",
    "                all_pass[gid] = \" \".join(all_sen)\n",
    "\n",
    "            full_dat_train[\"gold_pass\"].append(gid)\n",
    "            full_dat_train[\"gold_sen\"].append(sid)\n",
    "            full_dat_train[\"all_pass\"].append(all_pass)\n",
    "            full_dat_train[\"all_sen\"].append(all_sen)\n",
    "            full_dat_train[\"response\"].append(dial[\"text\"])\n",
    "            full_dat_train[\"context_eou\"].append(pseudo[:-1])\n",
    "            full_dat_train[\"all_topic\"].append(knowledge)\n",
    "\n",
    "\n",
    "\n",
    "            pseudo += dial[\"text\"]\n",
    "            pseudo += \" <eou> \"\n",
    "            input += f\"{dial['speaker'][2:]}: {dial['text']} \"\n",
    "\n",
    "        else:\n",
    "            pseudo += dial[\"text\"]\n",
    "            pseudo += \" <eou> \"\n",
    "            input += f\"{dial['speaker'][2:]}: {dial['text']} \"\n",
    "            last_ut = dial['text']\n",
    "\n",
    "index = 0\n",
    "g7 = 0\n",
    "l7 = 0\n",
    "f = []\n",
    "f2 = []\n",
    "not_found = []\n",
    "full_dat_valid = {\n",
    "    \"gold_pass\": [],\n",
    "    \"all_pass\": [],\n",
    "    \"gold_sen\": [],\n",
    "    \"all_sen\": [],\n",
    "    \"last_ut\": [],\n",
    "    \"context\": [],\n",
    "    \"response\": [],\n",
    "    \"context_eou\": [],\n",
    "    \"all_topic\": [],\n",
    "}\n",
    "for i, conv in tqdm(enumerate(wow_data_valid)):\n",
    "\n",
    "    input = \"\"\n",
    "    last_ut = \"\"\n",
    "    pseudo = \"\"\n",
    "\n",
    "    for j, dial in enumerate(conv[\"dialog\"]):\n",
    "        if \"Wizard\" in dial[\"speaker\"] and j > 0:\n",
    "            # res_key = custom_kw_extractor_1.extract_keywords(dial[\"text\"])\n",
    "            # rk, res_key = key_exrt(res_key, dial[\"text\"])\n",
    "            # ut_key = custom_kw_extractor_1.extract_keywords(last_ut)\n",
    "            # uk, ut_key = key_exrt(ut_key, last_ut)\n",
    "\n",
    "            # masked_res = masker(dial[\"text\"], rk, pseudo)\n",
    "\n",
    "            # pr_in = f\"ut: {ut_key} res: {res_key}\"\n",
    "            # sr_in = f\"ut: {ut_key} res: {res_key}\"\n",
    "            # mpr_in = f\"ut: {last_ut} res: {masked_res}\"\n",
    "            # msr_in = f\"ut: {last_ut} res: {masked_res}\"\n",
    "            full_dat_valid[\"context\"].append(input + dial[\"speaker\"][2:] + \": \")\n",
    "            full_dat_valid[\"last_ut\"].append(last_ut)\n",
    "\n",
    "            response = dial[\"text\"]\n",
    "\n",
    "            knowledge = [list(k.keys())[0] for k in dial[\"retrieved_passages\"]]\n",
    "\n",
    "            if len(knowledge) < 7:\n",
    "                knowledge = search(response, 7)\n",
    "                l7 += 1\n",
    "            elif len(knowledge) > 7:\n",
    "                knowledge = knowledge[:7]\n",
    "                g7 += 1\n",
    "\n",
    "            try:\n",
    "                gold_pass = list(dial[\"checked_passage\"].values())[0]\n",
    "            except IndexError:\n",
    "                gold_pass = \"no_passages_used\"\n",
    "\n",
    "            try:\n",
    "                gold_sen = list(dial[\"checked_sentence\"].values())[0]\n",
    "            except IndexError:\n",
    "                gold_sen = \"no_passages_used\"\n",
    "\n",
    "            if gold_pass == \"no_passages_used\" and gold_sen != \"no_passages_used\":\n",
    "                gold_pass = \" \".join(list(dial[\"checked_sentence\"].keys())[0].split(\"_\")[1:-1])\n",
    "\n",
    "            try:\n",
    "                assert type(topic2passage[gold_pass]) == list\n",
    "            except:\n",
    "                gold_pass = \" \".join(list(dial[\"checked_sentence\"].keys())[0].split(\"_\")[1:-1])\n",
    "                try:\n",
    "                    assert type(topic2passage[gold_pass]) == list\n",
    "                except:\n",
    "                    gold_pass = \"no_passages_used\"\n",
    "                    gold_sen = \"no_passages_used\"\n",
    "                    f2.append((i, j))\n",
    "\n",
    "            if gold_pass not in knowledge and gold_pass != \"no_passages_used\":\n",
    "                idx = randint(0, 6)\n",
    "                knowledge[idx] = gold_pass\n",
    "\n",
    "            all_sen = topic2passage[gold_pass]\n",
    "\n",
    "\n",
    "            if gold_pass == \"no_passages_used\":\n",
    "                gid = float(\"-inf\")\n",
    "                sid = float(\"-inf\")\n",
    "            else:\n",
    "                gid = knowledge.index(gold_pass)\n",
    "                try:\n",
    "                    sid = topic2passage[gold_pass].index(gold_sen)\n",
    "                except:\n",
    "                    sid = len(all_sen)\n",
    "                    all_sen += [gold_sen]\n",
    "                    f.append((i, j))\n",
    "\n",
    "            all_pass = [\" \".join(topic2passage[t]) for t in knowledge]\n",
    "            if gid != float(\"-inf\"):\n",
    "                all_pass[gid] = \" \".join(all_sen)\n",
    "\n",
    "            full_dat_valid[\"gold_pass\"].append(gid)\n",
    "            full_dat_valid[\"gold_sen\"].append(sid)\n",
    "            full_dat_valid[\"all_pass\"].append(all_pass)\n",
    "            full_dat_valid[\"all_sen\"].append(all_sen)\n",
    "            full_dat_valid[\"response\"].append(dial[\"text\"])\n",
    "            full_dat_valid[\"context_eou\"].append(pseudo[:-1])\n",
    "            full_dat_valid[\"all_topic\"].append(knowledge)\n",
    "\n",
    "\n",
    "\n",
    "            pseudo += dial[\"text\"]\n",
    "            pseudo += \" <eou> \"\n",
    "            input += f\"{dial['speaker'][2:]}: {dial['text']} \"\n",
    "\n",
    "        else:\n",
    "            pseudo += dial[\"text\"]\n",
    "            pseudo += \" <eou> \"\n",
    "            input += f\"{dial['speaker'][2:]}: {dial['text']} \"\n",
    "            last_ut = dial['text']\n",
    "\n",
    "index = 0\n",
    "g7 = 0\n",
    "l7 = 0\n",
    "f = []\n",
    "f2 = []\n",
    "not_found = []\n",
    "full_dat_test = {\n",
    "    \"gold_pass\": [],\n",
    "    \"all_pass\": [],\n",
    "    \"gold_sen\": [],\n",
    "    \"all_sen\": [],\n",
    "    \"last_ut\": [],\n",
    "    \"context\": [],\n",
    "    \"response\": [],\n",
    "    \"context_eou\": [],\n",
    "    \"all_topic\": [],\n",
    "}\n",
    "for i, conv in tqdm(enumerate(wow_data_test)):\n",
    "\n",
    "    input = \"\"\n",
    "    last_ut = \"\"\n",
    "    pseudo = \"\"\n",
    "\n",
    "    for j, dial in enumerate(conv[\"dialog\"]):\n",
    "        if \"Wizard\" in dial[\"speaker\"] and j > 0:\n",
    "            # res_key = custom_kw_extractor_1.extract_keywords(dial[\"text\"])\n",
    "            # rk, res_key = key_exrt(res_key, dial[\"text\"])\n",
    "            # ut_key = custom_kw_extractor_1.extract_keywords(last_ut)\n",
    "            # uk, ut_key = key_exrt(ut_key, last_ut)\n",
    "\n",
    "            # masked_res = masker(dial[\"text\"], rk, pseudo)\n",
    "\n",
    "            # pr_in = f\"ut: {ut_key} res: {res_key}\"\n",
    "            # sr_in = f\"ut: {ut_key} res: {res_key}\"\n",
    "            # mpr_in = f\"ut: {last_ut} res: {masked_res}\"\n",
    "            # msr_in = f\"ut: {last_ut} res: {masked_res}\"\n",
    "            full_dat_test[\"context\"].append(input + dial[\"speaker\"][2:] + \": \")\n",
    "            full_dat_test[\"last_ut\"].append(last_ut)\n",
    "\n",
    "            response = dial[\"text\"]\n",
    "\n",
    "            knowledge = [list(k.keys())[0] for k in dial[\"retrieved_passages\"]]\n",
    "\n",
    "            if len(knowledge) < 7:\n",
    "                knowledge = search(response, 7)\n",
    "                l7 += 1\n",
    "            elif len(knowledge) > 7:\n",
    "                knowledge = knowledge[:7]\n",
    "                g7 += 1\n",
    "\n",
    "            try:\n",
    "                gold_pass = list(dial[\"checked_passage\"].values())[0]\n",
    "            except IndexError:\n",
    "                gold_pass = \"no_passages_used\"\n",
    "\n",
    "            try:\n",
    "                gold_sen = list(dial[\"checked_sentence\"].values())[0]\n",
    "            except IndexError:\n",
    "                gold_sen = \"no_passages_used\"\n",
    "\n",
    "            if gold_pass == \"no_passages_used\" and gold_sen != \"no_passages_used\":\n",
    "                gold_pass = \" \".join(list(dial[\"checked_sentence\"].keys())[0].split(\"_\")[1:-1])\n",
    "\n",
    "            try:\n",
    "                assert type(topic2passage[gold_pass]) == list\n",
    "            except:\n",
    "                gold_pass = \" \".join(list(dial[\"checked_sentence\"].keys())[0].split(\"_\")[1:-1])\n",
    "                try:\n",
    "                    assert type(topic2passage[gold_pass]) == list\n",
    "                except:\n",
    "                    gold_pass = \"no_passages_used\"\n",
    "                    gold_sen = \"no_passages_used\"\n",
    "                    f2.append((i, j))\n",
    "\n",
    "            if gold_pass not in knowledge and gold_pass != \"no_passages_used\":\n",
    "                idx = randint(0, 6)\n",
    "                knowledge[idx] = gold_pass\n",
    "\n",
    "            all_sen = topic2passage[gold_pass]\n",
    "\n",
    "\n",
    "            if gold_pass == \"no_passages_used\":\n",
    "                gid = float(\"-inf\")\n",
    "                sid = float(\"-inf\")\n",
    "            else:\n",
    "                gid = knowledge.index(gold_pass)\n",
    "                try:\n",
    "                    sid = topic2passage[gold_pass].index(gold_sen)\n",
    "                except:\n",
    "                    sid = len(all_sen)\n",
    "                    all_sen += [gold_sen]\n",
    "                    f.append((i, j))\n",
    "\n",
    "            all_pass = [\" \".join(topic2passage[t]) for t in knowledge]\n",
    "            if gid != float(\"-inf\"):\n",
    "                all_pass[gid] = \" \".join(all_sen)\n",
    "\n",
    "            full_dat_test[\"gold_pass\"].append(gid)\n",
    "            full_dat_test[\"gold_sen\"].append(sid)\n",
    "            full_dat_test[\"all_pass\"].append(all_pass)\n",
    "            full_dat_test[\"all_sen\"].append(all_sen)\n",
    "            full_dat_test[\"response\"].append(dial[\"text\"])\n",
    "            full_dat_test[\"context_eou\"].append(pseudo[:-1])\n",
    "            full_dat_test[\"all_topic\"].append(knowledge)\n",
    "\n",
    "\n",
    "\n",
    "            pseudo += dial[\"text\"]\n",
    "            pseudo += \" <eou> \"\n",
    "            input += f\"{dial['speaker'][2:]}: {dial['text']} \"\n",
    "\n",
    "        else:\n",
    "            pseudo += dial[\"text\"]\n",
    "            pseudo += \" <eou> \"\n",
    "            input += f\"{dial['speaker'][2:]}: {dial['text']} \"\n",
    "            last_ut = dial['text']\n",
    "\n",
    "wow = DatasetDict()\n",
    "wow[\"train\"] = Dataset.from_dict(full_dat_train)\n",
    "\n",
    "wow[\"valid\"] = Dataset.from_dict(full_dat_valid)\n",
    "\n",
    "wow[\"test\"] = Dataset.from_dict(full_dat_test)\n",
    "\n",
    "wow.save_to_disk(\"wow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe46c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rank_bm25'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rank_bm25'"
     ]
    }
   ],
   "source": [
    "# import rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0cb77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
