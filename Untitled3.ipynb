{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140587e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "wow = DatasetDict.load_from_disk(\"wow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05429cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10\n",
    "\n",
    "custom_kw_extractor_1 = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4758dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def masker(inp, mask, ref_in, ref_out):\n",
    "    op = inp\n",
    "    msk = []\n",
    "    j = 0\n",
    "    for i, w in enumerate(mask):\n",
    "        if w.lower() in ref_out.lower() and w.lower() not in ref_in.lower():\n",
    "            op = op.replace(w, f\"<extra_id_{j}>\")\n",
    "            j+=1\n",
    "            msk.append(w)\n",
    "\n",
    "\n",
    "    return op, \" \".join(msk)\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "def key_exrt(k_lst, strng):\n",
    "\n",
    "    keywords = []\n",
    "    k2 = []\n",
    "    if len(k_lst) == 0:\n",
    "        keywords += [token for token in en(strng) if\n",
    "              not token.is_punct\n",
    "              and not token.is_currency\n",
    "              and not token.is_digit\n",
    "              and not token.is_oov\n",
    "              and not token.is_space\n",
    "              and not token.is_stop\n",
    "              and not token.like_num\n",
    "              and not token.pos_ in []]\n",
    "    else:\n",
    "        keywords += [key for key, _ in k_lst]\n",
    "        k2 += [key for key, _ in k_lst]\n",
    "\n",
    "    return keywords, (\"; \".join(keywords) + \";\"), k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6327bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep1(df):\n",
    "    out = {\"gold_pass\": [], \"last_ut\": [], \"all_pass\": [], \"all_topic\": [], \"all_sen\": [], \"response\": [], \"context_eou\": []}\n",
    "    for i, _ in enumerate(df[\"gold_pass\"]):\n",
    "        # print(type(k))\n",
    "        if df[\"gold_pass\"][i] == float(\"-inf\"):\n",
    "            continue\n",
    "        for k in out.keys():\n",
    "            out[k].append(df[k][i])\n",
    "    return out\n",
    "wow = wow.map(prep1, batched=True, remove_columns=wow[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7e796e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['gold_pass', 'all_pass', 'all_sen', 'last_ut', 'response', 'context_eou', 'all_topic'],\n",
       "        num_rows: 70188\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['gold_pass', 'all_pass', 'all_sen', 'last_ut', 'response', 'context_eou', 'all_topic'],\n",
       "        num_rows: 3759\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['gold_pass', 'all_pass', 'all_sen', 'last_ut', 'response', 'context_eou', 'all_topic'],\n",
       "        num_rows: 3676\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addbc9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ford Motor Company'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = 2333\n",
    "# wow[\"train\"][\"all_topic\"][i][int(wow[\"train\"][\"gold_pass\"][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd1055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c0ce6c06354e45885dcf4d88f37ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73cc625a27048e4ba57395951fd381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7221ae83e37a4776a63eda09b3c3155d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prep2(df):\n",
    "    out = {\"output\": [], \"input\": []}\n",
    "    for i, _ in enumerate(df[\"gold_pass\"]):\n",
    "        keys = custom_kw_extractor_1.extract_keywords(df[\"response\"][i])\n",
    "        key, strng, js_ks = key_exrt(keys, df[\"response\"][i])\n",
    "        pas = df[\"all_pass\"][i][int(df[\"gold_pass\"][i])] if df[\"gold_pass\"][i] != float(\"-inf\") else \"\"\n",
    "        jk, gpe_out = masker(df[\"response\"][i], js_ks, df[\"context_eou\"][i].replace(\" <eou>\", \"\"), pas)\n",
    "        title = df[\"all_topic\"][i][int(df[\"gold_pass\"][i])] if df[\"gold_pass\"][i] != float(\"-inf\") else \"\"\n",
    "        q = df[\"last_ut\"][i] + \" <eou> \" + jk\n",
    "        input_1 = f\"question: {q} title: {title} passage: {pas}\"\n",
    "        out[\"output\"].append(gpe_out)\n",
    "        out[\"input\"].append(input_1)\n",
    "    return out\n",
    "wow_in_out = wow.map(prep2, batched=True, remove_columns=wow[\"train\"].column_names)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c81d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input'],\n",
       "        num_rows: 70188\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['output', 'input'],\n",
       "        num_rows: 3759\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['output', 'input'],\n",
       "        num_rows: 3676\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wow_in_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81159f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:199: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "question = \"question:\"\n",
    "title = \"title:\"\n",
    "context = \"passage:\"\n",
    "eou = \"<eou>\"\n",
    "tokenizer.add_tokens([question, title, context, eou], special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5daedf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aadf0f30fc94fca8dce4224119dfd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6d4966357d4e41a197e25f18e98005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbec57d8f3d49bf8d5ce45241982307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(df):\n",
    "    return tokenizer(df[\"input\"], max_length=512, truncation=True, padding=False)\n",
    "wow_in_out = wow_in_out.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c46951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75e21ea2a6e48e58bef6a23ae878ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e96328f5bd64c8e90443b65950004c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354a3bf65f2545e48fa59e44231997c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_labels(df):\n",
    "    out = tokenizer(df[\"output\"], truncation=False, padding=False)\n",
    "    return {\"labels\": out[\"input_ids\"]}\n",
    "wow_in_out = wow_in_out.map(tokenize_labels, batched=True, remove_columns=[\"output\", \"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b8bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eac565d11ce4c7ca9de8d3aeb79c564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/70188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eb70f3db0d40be95e03593a022707c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecb07168e1448e79b813389c3b4f05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wow_in_out.save_to_disk(\"wow_pretraining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4316511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 29 12:43:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    60W / 300W |   5048MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    57W / 300W |  31376MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    59W / 300W |  18572MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    58W / 300W |  20298MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    61W / 300W |   4583MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    58W / 300W |  31688MiB / 32510MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    78W / 300W |  11504MiB / 32510MiB |     22%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    57W / 300W |  32032MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2195MiB |\n",
      "|    0   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2057MiB |\n",
      "|    0   N/A  N/A   3908879      C   python3                           785MiB |\n",
      "|    1   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2357MiB |\n",
      "|    1   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    1   N/A  N/A   3004709      C   .../envs/minigpt4/bin/python    26793MiB |\n",
      "|    2   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2347MiB |\n",
      "|    2   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    2   N/A  N/A   3626570      C   ...onda3/envs/wow/bin/python    13999MiB |\n",
      "|    3   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2357MiB |\n",
      "|    3   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    3   N/A  N/A   2590979      C   python                          15715MiB |\n",
      "|    4   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2357MiB |\n",
      "|    4   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    5   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2211MiB |\n",
      "|    5   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    5   N/A  N/A   3958086      C   python                          27251MiB |\n",
      "|    6   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    6   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    6   N/A  N/A   3908879      C   python3                          7053MiB |\n",
      "|    7   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     8425MiB |\n",
      "|    7   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     8425MiB |\n",
      "|    7   N/A  N/A   3488074      C   .../envs/work_env/bin/python    15177MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad7966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
