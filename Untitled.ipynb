{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c776477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 26 19:22:09 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    60W / 300W |  31689MiB / 32510MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    58W / 300W |  31376MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    59W / 300W |  26766MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    59W / 300W |  20298MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    61W / 300W |   4583MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    59W / 300W |   4437MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    61W / 300W |   4449MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    57W / 300W |  16855MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2195MiB |\n",
      "|    0   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2057MiB |\n",
      "|    0   N/A  N/A   1535447      C   python3                           787MiB |\n",
      "|    0   N/A  N/A   1639920      C   .../thesis_trial1/bin/python    26645MiB |\n",
      "|    1   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2357MiB |\n",
      "|    1   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    1   N/A  N/A   3004709      C   .../envs/minigpt4/bin/python    26793MiB |\n",
      "|    2   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2347MiB |\n",
      "|    2   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    2   N/A  N/A   1535447      C   python3                         22191MiB |\n",
      "|    3   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2357MiB |\n",
      "|    3   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    3   N/A  N/A   2590979      C   python                          15715MiB |\n",
      "|    4   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2357MiB |\n",
      "|    4   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    5   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2211MiB |\n",
      "|    5   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    6   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    6   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     2221MiB |\n",
      "|    7   N/A  N/A    255937      C   .../envs/minigpt4/bin/python     8425MiB |\n",
      "|    7   N/A  N/A    327807      C   .../envs/minigpt4/bin/python     8425MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16956977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c032c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "qg_wow = DatasetDict.load_from_disk(\"wow_qg_large\")\n",
    "rank_wow = DatasetDict.load_from_disk(\"wow_rank_kel_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d74e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crt(**kwargs):\n",
    "    d = {}\n",
    "    for k in kwargs.keys():\n",
    "        d[k] = []\n",
    "    return d\n",
    "    \n",
    "def apnd(d, i, **kwargs):\n",
    "#     print(kwargs)\n",
    "    for k in kwargs.keys():\n",
    "#         print(type(k))\n",
    "#         print(kwargs[k][i])\n",
    "        d[k].append(kwargs[k][i])\n",
    "    \n",
    "    return d\n",
    "\n",
    "def prep(df):\n",
    "    out = crt(**df)\n",
    "#     print(out)\n",
    "    \n",
    "    for i, jk in enumerate(df[\"gold_pass\"]):\n",
    "        if jk != float(\"-inf\"):\n",
    "            out = apnd(out, i, **df)\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2301b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_wowd =  qg_wow.map(prep, batched=True, remove_columns=qg_wow[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7177be42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['gold_pass', 'all_pass', 'gold_sen', 'all_sen', 'last_ut', 'context', 'response', 'context_eou', 'all_topic', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 70188\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['gold_pass', 'all_pass', 'gold_sen', 'all_sen', 'last_ut', 'context', 'response', 'context_eou', 'all_topic', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 3759\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['gold_pass', 'all_pass', 'gold_sen', 'all_sen', 'last_ut', 'context', 'response', 'context_eou', 'all_topic', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 3676\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qg_wowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b51ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"wow_rank_t5-base/final\")\n",
    "question = \"question:\"\n",
    "title = \"title:\"\n",
    "context = \"context:\"\n",
    "eou = \"<eou>\"\n",
    "tokenizer.add_tokens([question, title, context, eou], special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175af401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "query_tokenizer = T5Tokenizer.from_pretrained(\"wow_qg_t5-large/final\")\n",
    "# query_tokenizer.decode(qg_wowd[\"test\"][\"labels\"][175])\n",
    "qg_model = T5ForConditionalGeneration.from_pretrained(\"wow_qg_t5-large/final\")\n",
    "# qg_model.generate(torch.tensor(qg_wowd[\"test\"][\"input_ids\"][175]), torch.tensor(qg_wowd[\"test\"][\"attention_mask\"][175]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef69822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Target is a<extra_id_1><extra_id_2><extra_id_3> in<extra_id_0>.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor(qg_wowd[\"test\"][\"input_ids\"][134]).view(1, -1)\n",
    "att = torch.tensor(qg_wowd[\"test\"][\"attention_mask\"][134]).view(1, -1)\n",
    "query_tokenizer.decode(qg_model.generate(input_ids=inp, attention_mask=att, num_beams=5, max_length=128).view(-1)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da02e461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wizard: The Duramax is a V8 engine for trucks Apprentice: I do not know much about vehicles. I know that a V8 is very powerful. Wizard: The Duramax was originally installed in 2001 and is an option for trucks. Apprentice: I remember the commercials for Duramax. I believe the jingle was \"Built to last, Duralast\". Do you remember that? Wizard: Ha, yes. Originally, customers complained of severe overheating. Apprentice: That sounds very bad. Was this something that was fixed? Wizard: </s>'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokenizer.decode(qg_wowd[\"test\"][\"input_ids\"][523])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39239791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GM<extra_id_2> the<extra_id_0>, but<extra_id_3> after being<extra_id_1>.</s>'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokenizer.decode(qg_wowd[\"test\"][\"labels\"][523])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "835c1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"llm\".replace(\"<nok>\", \"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f18acf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "qg_model = qg_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b68fad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep1(df):\n",
    "    out = {\"pass_label\": [], \"attention_mask\": [], \"input_ids\": []}\n",
    "    for i, _ in enumerate(df[\"gold_pass\"]):\n",
    "        # print(type(k))\n",
    "        if df[\"gold_pass\"][i] == float(\"-inf\"):\n",
    "            continue\n",
    "        inp = torch.tensor(df[\"input_ids\"][i]).view(1, -1).to(device)\n",
    "        att = torch.tensor(df[\"attention_mask\"][i]).view(1, -1).to(device)\n",
    "        input_q = query_tokenizer.decode(qg_model.generate(input_ids=inp, attention_mask=att, num_beams=5, max_length=128).view(-1)[1:]).replace(\"<nok>\", \"\")\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "#         out[\"pass_label\"].append(int(df[\"gold_pass\"][i]))\n",
    "        for j, p in enumerate(df[\"all_pass\"][i]):\n",
    "\n",
    "            if j == int(df[\"gold_pass\"][i]):\n",
    "                # pass_label.append(1)\n",
    "                p = (\" \".join(df[\"all_sen\"][i]))\n",
    "\n",
    "            p = p.replace(\"no_passages_used\", \"\")\n",
    "            t = df[\"all_topic\"][i][j]\n",
    "            inp = tokenizer(f\"question: {input_q} title: {t} passage: {p}\", max_length=512, truncation=True, padding=False)\n",
    "            out[\"input_ids\"].append(inp[\"input_ids\"])\n",
    "            out[\"attention_mask\"].append(inp[\"attention_mask\"])\n",
    "            \n",
    "            out[\"pass_label\"].append(int(df[\"gold_pass\"][i]))\n",
    "#             out[\"labels\"].append(tokenizer(gpe_out)[\"input_ids\"])\n",
    "#         out[\"input_ids\"].append(torch.tensor(input_ids))\n",
    "#         out[\"attention_mask\"].append(torch.tensor(attention_mask))\n",
    "        # out[\"masked_response\"].append(input_q)\n",
    "        # out[\"resp\"].append(df[\"response\"][i])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29846ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1112724a29144c93850e1c25a815be60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rank_data = qg_wowd[\"test\"].map(prep1, batched=True, remove_columns=qg_wowd[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912063f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_data.save_to_disk(\"wow_qg_test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb257536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are there organizations or anything that you could belong to if you were a genius?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43554041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qg_wow = qg_wow.remove_columns(['gold_pass', 'all_pass', 'gold_sen', 'all_sen', 'last_ut', 'context', 'response', 'context_eou', 'all_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d281c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pass_label', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 491316\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pass_label', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 26313\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pass_label', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 25732\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_wow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960760cf",
   "metadata": {},
   "source": [
    "# Ranker Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131363a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import T5ForConditionalGeneration, T5Config, T5Tokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import datasets\n",
    "ranker_ckpt = \"wow_rank_t5-base/final\"\n",
    "class RankT5GPE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "        config.rank_score_index = 32019\n",
    "        config.n_pass = 7\n",
    "        config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "        self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "#             print(decoder_input_ids.size())\n",
    "# <pad> k1 k2 k3 \n",
    "#       -1 -2 4 --> 1\n",
    "#            rs --> 2\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "        rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :]) # bn * dsl * 512 --> bn * 512 --> bn * v \n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass) # bn * v --> bn --> b * n\n",
    "\n",
    "\n",
    "        if labels != None:\n",
    "            logits = out.logits\n",
    "            batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = (labels_flat != -100)\n",
    "            arry = torch.arange(batch_size_n * sequence_length).to(logits_flat.device)\n",
    "\n",
    "            selected_logits = logits_flat[arry[mask], labels_flat[mask]]\n",
    "            output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "            output_logits[mask] = selected_logits\n",
    "\n",
    "            output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "            out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "        else:\n",
    "            out.gpe_score = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass] # bn\n",
    "            # 3 --> log_softmax(rank_score)[3]\n",
    "            # Q -> [23, 34, 48, 32] pl -> [2] rs -> [-2, -3, 5, -1] -> [_, _, __, _]\n",
    "            # Q -> [23, 48, 34, 32] pl -> [2] rs -> [-2, 5, -3, -1] -> [_, _, __, _]\n",
    "            rank_score = out.rank_score # [-2, 5, -3, -1] \n",
    "            gen_score = out.gpe_score # [-3, 4, -2, -1]\n",
    "\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct1(rank_score, pass_label.view(-1))\n",
    "            gen_loss = loss_fct2(gen_score, pass_label.view(-1))\n",
    "\n",
    "            loss = rank_loss + gen_loss\n",
    "            out.loss = loss\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6357e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_ckp = \"t5-base\"\n",
    "rank_tokenizer = T5Tokenizer.from_pretrained(ranker_ckpt)\n",
    "question = \"question:\"\n",
    "title = \"title:\"\n",
    "context = \"context:\"\n",
    "eou = \"<eou>\"\n",
    "rank_tokenizer.add_tokens([question, title, context, eou], special_tokens=True)\n",
    "ranker_config = T5Config.from_pretrained(ranker_ckpt)\n",
    "ranker_config.output_hidden_states = True\n",
    "ranker_config.rank_score_index = rank_tokenizer.convert_tokens_to_ids(\"<extra_id_80>\")\n",
    "ranker_config.n_pass = 7\n",
    "ranker_config.temp1 = 1\n",
    "ranker_config.temp2 = 1\n",
    "ranker = RankT5GPE.from_pretrained(ranker_ckpt, config=ranker_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443334ba",
   "metadata": {},
   "source": [
    "# Query Gen Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbb19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qgen_ckpt = \"wow_qg_t5-large/final\"\n",
    "qgen = T5ForConditionalGeneration.from_pretrained(qgen_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f7601ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qgen_tokenizer = T5Tokenizer.from_pretrained(qgen_ckpt)\n",
    "# wizard = \"Wizard:\"\n",
    "# apprentice = \"Apprentice:\"\n",
    "# nok = \"<nok>\"\n",
    "# qgen_tokenizer.add_tokens([wizard, apprentice, nok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1(df):\n",
    "    out = {\"pass_label\": [], \"attention_mask\": [], \"input_ids\": []}\n",
    "    for i, _ in enumerate(df[\"gold_pass\"]):\n",
    "        # print(type(k))\n",
    "        if df[\"gold_pass\"][i] == float(\"-inf\"):\n",
    "            continue\n",
    "#         keys = custom_kw_extractor_1.extract_keywords(df[\"response\"][i])\n",
    "#         key, strng, js_ks = key_exrt(keys, df[\"response\"][i])\n",
    "#         pas = df[\"all_pass\"][i][int(df[\"gold_pass\"][i])] if df[\"gold_pass\"][i] != float(\"-inf\") else \"\"\n",
    "#         msk_res, gpe_out = masker(df[\"response\"][i], js_ks, df[\"context_eou\"][i].replace(\" <eou>\", \"\"), pas)\n",
    "        input_q = df[\"last_ut\"][i] \n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "#         out[\"pass_label\"].append(int(df[\"gold_pass\"][i]))\n",
    "        for j, p in enumerate(df[\"all_pass\"][i]):\n",
    "\n",
    "            if j == int(df[\"gold_pass\"][i]):\n",
    "                # pass_label.append(1)\n",
    "                p = (\" \".join(df[\"all_sen\"][i]))\n",
    "\n",
    "            p = p.replace(\"no_passages_used\", \"\")\n",
    "            t = df[\"all_topic\"][i][j]\n",
    "            inp = tokenizer(f\"question: {input_q} title: {t} passage: {p}\", max_length=512, truncation=True, padding=False)\n",
    "            out[\"input_ids\"].append(inp[\"input_ids\"])\n",
    "            out[\"attention_mask\"].append(inp[\"attention_mask\"])\n",
    "        \n",
    "    #         le = 0\n",
    "    #         for k in input_ids:\n",
    "    #             if len(k) > le:\n",
    "    #                 le = len(k)\n",
    "\n",
    "    #         input_ids = [m + [tokenizer.pad_token_id]*(le-len(m)) for m in input_ids]\n",
    "    #         attention_mask = [m + [tokenizer.pad_token_id]*(le-len(m)) for m in attention_mask]\n",
    "            \n",
    "            out[\"pass_label\"].append(int(df[\"gold_pass\"][i]))\n",
    "#             out[\"labels\"].append(tokenizer(gpe_out)[\"input_ids\"])\n",
    "#         out[\"input_ids\"].append(torch.tensor(input_ids))\n",
    "#         out[\"attention_mask\"].append(torch.tensor(attention_mask))\n",
    "        # out[\"masked_response\"].append(input_q)\n",
    "        # out[\"resp\"].append(df[\"response\"][i])\n",
    "\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
