{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7121ee43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 12 06:32:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    58W / 300W |  10270MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    56W / 300W |  26798MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   50C    P0   157W / 300W |   9795MiB / 32510MiB |     87%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   39C    P0   179W / 300W |  22809MiB / 32510MiB |     83%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   24C    P0    44W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   47C    P0   248W / 300W |  13160MiB / 32510MiB |     97%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   61C    P0   258W / 300W |  28276MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   276W / 300W |  24116MiB / 32510MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3778289      C   ...onda3/envs/wow/bin/python    10265MiB |\n",
      "|    1   N/A  N/A   3004709      C   .../envs/minigpt4/bin/python    26793MiB |\n",
      "|    2   N/A  N/A   1598486      C   python3                          2683MiB |\n",
      "|    2   N/A  N/A   2335981      C   python                           7109MiB |\n",
      "|    3   N/A  N/A   2335981      C   python                           7089MiB |\n",
      "|    3   N/A  N/A   2590979      C   python                          15715MiB |\n",
      "|    5   N/A  N/A   3096378      C   python                          13157MiB |\n",
      "|    6   N/A  N/A   3628578      C   python                          28269MiB |\n",
      "|    7   N/A  N/A   3654015      C   python                          24109MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3e8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3856af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "wow = DatasetDict.load_from_disk(\"wow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0eeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10\n",
    "\n",
    "custom_kw_extractor_1 = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35defe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def masker(inp, mask, ref_in, ref_out):\n",
    "    op = inp\n",
    "    msk = []\n",
    "    j = 0\n",
    "    for i, w in enumerate(mask):\n",
    "        if w.lower() in ref_out.lower() and w.lower() not in ref_in.lower():\n",
    "            op = op.replace(w, f\"<extra_id_{j}>\")\n",
    "            j+=1\n",
    "            msk.append(w)\n",
    "\n",
    "\n",
    "    return op, \" \".join(msk)\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "def key_exrt(k_lst, strng):\n",
    "\n",
    "    keywords = []\n",
    "    k2 = []\n",
    "    if len(k_lst) == 0:\n",
    "        keywords += [token for token in en(strng) if\n",
    "              not token.is_punct\n",
    "              and not token.is_currency\n",
    "              and not token.is_digit\n",
    "              and not token.is_oov\n",
    "              and not token.is_space\n",
    "              and not token.is_stop\n",
    "              and not token.like_num\n",
    "              and not token.pos_ in []]\n",
    "    else:\n",
    "        keywords += [key for key, _ in k_lst]\n",
    "        k2 += [key for key, _ in k_lst]\n",
    "\n",
    "    return keywords, (\"; \".join(keywords) + \";\"), k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51802404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:199: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "question = \"question:\"\n",
    "title = \"title:\"\n",
    "context = \"context:\"\n",
    "eou = \"<eou>\"\n",
    "tokenizer.add_tokens([question, title, context, eou], special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef75493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def prep1(df):\n",
    "    out = {\"pass_label\": [], \"attention_mask\": [], \"input_ids\": []}\n",
    "    for i, _ in enumerate(df[\"gold_pass\"]):\n",
    "        # print(type(k))\n",
    "        if df[\"gold_pass\"][i] == float(\"-inf\"):\n",
    "            continue\n",
    "#         keys = custom_kw_extractor_1.extract_keywords(df[\"response\"][i])\n",
    "#         key, strng, js_ks = key_exrt(keys, df[\"response\"][i])\n",
    "#         pas = df[\"all_pass\"][i][int(df[\"gold_pass\"][i])] if df[\"gold_pass\"][i] != float(\"-inf\") else \"\"\n",
    "#         msk_res, gpe_out = masker(df[\"response\"][i], js_ks, df[\"context_eou\"][i].replace(\" <eou>\", \"\"), pas)\n",
    "        input_q = df[\"last_ut\"][i] \n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "#         out[\"pass_label\"].append(int(df[\"gold_pass\"][i]))\n",
    "        for j, p in enumerate(df[\"all_pass\"][i]):\n",
    "\n",
    "            if j == int(df[\"gold_pass\"][i]):\n",
    "                # pass_label.append(1)\n",
    "                p = (\" \".join(df[\"all_sen\"][i]))\n",
    "\n",
    "            p = p.replace(\"no_passages_used\", \"\")\n",
    "            t = df[\"all_topic\"][i][j]\n",
    "            inp = tokenizer(f\"question: {input_q} title: {t} passage: {p}\", max_length=512, truncation=True, padding=False)\n",
    "            out[\"input_ids\"].append(inp[\"input_ids\"])\n",
    "            out[\"attention_mask\"].append(inp[\"attention_mask\"])\n",
    "        \n",
    "    #         le = 0\n",
    "    #         for k in input_ids:\n",
    "    #             if len(k) > le:\n",
    "    #                 le = len(k)\n",
    "\n",
    "    #         input_ids = [m + [tokenizer.pad_token_id]*(le-len(m)) for m in input_ids]\n",
    "    #         attention_mask = [m + [tokenizer.pad_token_id]*(le-len(m)) for m in attention_mask]\n",
    "            \n",
    "            out[\"pass_label\"].append(int(df[\"gold_pass\"][i]))\n",
    "#             out[\"labels\"].append(tokenizer(gpe_out)[\"input_ids\"])\n",
    "#         out[\"input_ids\"].append(torch.tensor(input_ids))\n",
    "#         out[\"attention_mask\"].append(torch.tensor(attention_mask))\n",
    "        # out[\"masked_response\"].append(input_q)\n",
    "        # out[\"resp\"].append(df[\"response\"][i])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8dd3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d550692ace748cb990f55338dd69e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74092 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07f43fe32df433ab4d36bbda337cb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3939 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43352d6eed7d4033bfea9d69cdaad154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wow = wow.map(prep1, batched=True, remove_columns=wow[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_wow.rename_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b536eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1147769cebb44eb68ad42a5c6f358fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/491316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2ba132c4ba4befbb31b5129ed9a09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789d1f4e36ac4b49b39604bfba2b3c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wow.save_to_disk(\"wow_rank_ut_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8081a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wow.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"pass_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89c69c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pass_label', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 491316\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pass_label', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 26313\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pass_label', 'labels', 'attention_mask', 'input_ids'],\n",
       "        num_rows: 25732\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c7f610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1]),\n",
       " tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1]),\n",
       " tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1]),\n",
       " tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1]),\n",
       " tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1]),\n",
       " tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1]),\n",
       " tensor([  569,  1827, 22466, 17201,    18,    89,    23,     1])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wow[\"train\"][\"labels\"][:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb58db7",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b187ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RankT5GPE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "        config.rank_score_index = None\n",
    "        config.n_pass = None\n",
    "        super().__init__(config)\n",
    "        self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = batch_size_n/self.n_pass\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "#             print(decoder_input_ids.size())\n",
    "\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "        rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :])\n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass)\n",
    "\n",
    "\n",
    "        if labels != None:\n",
    "            logits = out.logits\n",
    "            batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = (labels_flat != -100)\n",
    "\n",
    "            selected_logits = logits_flat[torch.arange(batch_size_n * sequence_length)[mask], labels_flat[mask]]\n",
    "            output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "            output_logits[mask] = selected_logits\n",
    "\n",
    "            output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "            out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "        else:\n",
    "            out.gpe_score = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass]\n",
    "            rank_score = out.rank_score\n",
    "            gen_score = out.gpe_score\n",
    "\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct1(rank_score, pass_label.view(-1))\n",
    "            gen_loss = loss_fct2(gen_score, pass_label.view(-1))\n",
    "\n",
    "            loss = rank_loss + gen_loss\n",
    "            out.loss = loss\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "855a8ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pass_label = torch.randint(0, 5, (2,))\n",
    "pass_label.view(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d0fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randint(0, 32127, (10, 10))\n",
    "lab = torch.randint(0, 32127, (2, 6))\n",
    "lab = lab.view(2, 1, 6).contiguous()\n",
    "lab = lab.expand(2, 5, 6).contiguous()\n",
    "lab = lab.view(10, 6)\n",
    "pass_label = torch.randint(0, 5, (2,))\n",
    "pass_label = pass_label.view(2, 1)\n",
    "pass_label = pass_label.expand(2, 5).contiguous()\n",
    "pass_label = pass_label.view(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67586013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pass_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e73fbe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RankT5GPE were not initialized from the model checkpoint at t5-small and are newly initialized: ['rank_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "mod_ckp = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(mod_ckp)\n",
    "config = T5Config.from_pretrained(mod_ckp)\n",
    "model = RankT5GPE(config).from_pretrained(mod_ckp)\n",
    "model.config.output_hidden_states = True\n",
    "model.config.rank_score_index = tokenizer.convert_tokens_to_ids(\"<extra_id_80>\")\n",
    "model.config.n_pass = 7\n",
    "\n",
    "# x = m(input_ids=logits, attention_mask=torch.ones_like(logits), pass_label=pass_label, labels=lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffce2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "batch_size = 8*model.config.n_pass \n",
    "model_dir = f\"wow_rank_{mod_ckp}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7164b139",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer)\n\u001b[0;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_wow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_wow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:56\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m ):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer.py:541\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    539\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[1;32m    540\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m \u001b[43mCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer_callback.py:296\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer_callback.py:313\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[0;32m--> 313\u001b[0m     cb \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[1;32m    314\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/integrations.py:585\u001b[0m, in \u001b[0;36mTensorBoardCallback.__init__\u001b[0;34m(self, tb_writer)\u001b[0m\n\u001b[1;32m    583\u001b[0m has_tensorboard \u001b[38;5;241m=\u001b[39m is_tensorboard_available()\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tensorboard:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m install tensorboardX.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m     )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX."
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_wow[\"train\"],\n",
    "    eval_dataset=tokenized_wow[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fbb6604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚ñÅCro'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abee851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"question: Oh that's awesome. I have a corgi who is a year old. She's very well behaved and easily trainable. <eou> That is a dog<extra_id_2> to mines I have a<extra_id_1> corgi it is a much smaller dog though<extra_id_3> from<extra_id_0>. title: Cardigan Welsh Corgi passage: The Cardigan Welsh Corgi is one of two separate dog breeds known as Welsh corgis that originated in Wales, predating the other breed, the Pembroke Welsh Corgi. It is one of the oldest breeds of the British Isles. Cardigan Welsh Corgis are known to be an extremely loyal dog breed. They are also versatile and can live in a variety of settings. However, they benefit from regular physical and mental stimulation. Pembrokes and Cardigans first appeared together in dog shows in 1925 when they were shown under the rules of The Kennel Club in Britain. The Corgi Club was founded in December, 1925 in Carmarthen in South Wales.</s>\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_wow[\"input_ids\"][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e37d145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32103"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<eou>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e5f512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_99>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(31999+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5ed0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
