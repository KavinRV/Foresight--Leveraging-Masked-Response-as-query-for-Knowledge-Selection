{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873b076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep  2 08:05:00 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    77W / 300W |  10439MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   51C    P0   243W / 300W |   9551MiB / 32510MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   39C    P0   206W / 300W |   2448MiB / 32510MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0   137W / 300W |   2424MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    94W / 300W |   2400MiB / 32510MiB |     17%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    44W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    44W / 300W |      7MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    65W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3870616      C   python3                          3333MiB |\n",
      "|    0   N/A  N/A   4051951      C   python3                          7095MiB |\n",
      "|    1   N/A  N/A   3917244      C   python3                          2443MiB |\n",
      "|    1   N/A  N/A   4029609      C   python3                          7095MiB |\n",
      "|    2   N/A  N/A   3917244      C   python3                          2445MiB |\n",
      "|    3   N/A  N/A   3917244      C   python3                          2421MiB |\n",
      "|    4   N/A  N/A   3917244      C   python3                          2397MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82cea2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d235fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "# from train_kle_bse import RankT5GPE\n",
    "from torch import nn\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c82a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wow = datasets.load_from_disk(\"wow_qg_test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8aad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_wow[\"valid\"] = Dataset.from_dict(tokenized_wow[\"valid\"][:-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250bb34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'pass_label'],\n",
       "    num_rows: 25732\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0c87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"wow_rank_t5-base_sym_kld_tem:T1S1/final\"\n",
    "# model_ckpt = \"wow_kle_t5_base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66ead09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_inp(df):\n",
    "    out = {\"decoder_input_ids\": []}\n",
    "    for i, _ in enumerate(df[\"pass_label\"]):\n",
    "        out[\"decoder_input_ids\"].append(tokenizer(\"<pad>\", add_special_tokens=False)[\"input_ids\"])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ba1aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c8240679f84d009722a26f9433bb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test = tokenized_wow[\"test\"].map(dec_inp, batched=True)\n",
    "# test = tokenized_wow[\"test\"]\n",
    "test = tokenized_wow.map(dec_inp, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05584399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: Cinematography is a type of motion picture, captured electronically by means of an image-sensor, produces an electrical charge</s> title: Polarization (waves) passage: Polarization (also polarisation) is a property applying to transverse waves that specifies the geometrical orientation of the oscillations. In a transverse wave, the direction of the oscillation is transverse to the direction of motion of the wave, so the oscillations can have different directions perpendicular to the wave direction. A simple example of a polarized transverse wave is vibrations traveling along a taut string \"(see image)\"; for example, in a musical instrument like a guitar string. Depending on how the string is plucked, the vibrations can be in a vertical direction, horizontal direction, or at any angle perpendicular to the string.</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test[\"input_ids\"][34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604f7009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ae1ccd59e64239a14fea166b194ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def p2p(df):\n",
    "    out = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for i, inpid in enumerate(df[\"input_ids\"]):\n",
    "        ide = inpid.index(1)\n",
    "        out[\"input_ids\"].append(inpid[:ide] + inpid[ide+1:])\n",
    "        att = df[\"attention_mask\"][i]\n",
    "        out[\"attention_mask\"].append(att[:ide] + att[ide+1:])\n",
    "    return out\n",
    "test = test.map(p2p, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49caa31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: Cinematography is a type of motion picture, captured electronically by means of an image-sensor, produces an electrical charge title: Polarization (waves) passage: Polarization (also polarisation) is a property applying to transverse waves that specifies the geometrical orientation of the oscillations. In a transverse wave, the direction of the oscillation is transverse to the direction of motion of the wave, so the oscillations can have different directions perpendicular to the wave direction. A simple example of a polarized transverse wave is vibrations traveling along a taut string \"(see image)\"; for example, in a musical instrument like a guitar string. Depending on how the string is plucked, the vibrations can be in a vertical direction, horizontal direction, or at any angle perpendicular to the string.</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test[\"input_ids\"][34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a258deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankT5GPE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "        config.rank_score_index = 32019\n",
    "        config.n_pass = 7\n",
    "        config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "        self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "#             print(decoder_input_ids.size())\n",
    "# <pad> k1 k2 k3 \n",
    "#       -1 -2 4 --> 1\n",
    "#            rs --> 2\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "        rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :]) # bn * dsl * 512 --> bn * 512 --> bn * v \n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass) # bn * v --> bn --> b * n\n",
    "\n",
    "\n",
    "        if labels != None:\n",
    "            logits = out.logits\n",
    "            batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = (labels_flat != -100)\n",
    "            arry = torch.arange(batch_size_n * sequence_length).to(logits_flat.device)\n",
    "\n",
    "            selected_logits = logits_flat[arry[mask], labels_flat[mask]]\n",
    "            output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "            output_logits[mask] = selected_logits\n",
    "\n",
    "            output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "            out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "        else:\n",
    "            out.gpe_score = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass] # bn\n",
    "            # 3 --> log_softmax(rank_score)[3]\n",
    "            # Q -> [23, 34, 48, 32] pl -> [2] rs -> [-2, -3, 5, -1] -> [_, _, __, _]\n",
    "            # Q -> [23, 48, 34, 32] pl -> [2] rs -> [-2, 5, -3, -1] -> [_, _, __, _]\n",
    "            rank_score = out.rank_score # [-2, 5, -3, -1] \n",
    "            gen_score = out.gpe_score # [-3, 4, -2, -1]\n",
    "\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct1(rank_score, pass_label.view(-1))\n",
    "            gen_loss = loss_fct2(gen_score, pass_label.view(-1))\n",
    "\n",
    "            loss = rank_loss + gen_loss\n",
    "            out.loss = loss\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68edf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "class RankT5(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "        config.rank_score_index = 32019\n",
    "        config.n_pass = 7\n",
    "        config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "#         self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "        labels = None\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        if decoder_input_ids == None and labels == None:\n",
    "            decoder_input_ids = torch.zeros((batch_size_n, 1), dtype=int).to(input_ids.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "            print(decoder_input_ids.size())\n",
    "\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "#         rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :])\n",
    "        rank_score = out.logits[:, 0, :] \n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass)\n",
    "        \n",
    "\n",
    "\n",
    "#         if labels != None:\n",
    "#             logits = out.logits\n",
    "#             batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "#             logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "#             labels_flat = labels.view(-1)\n",
    "#             mask = (labels_flat != -100)\n",
    "#             arry = torch.arange(batch_size_n * sequence_length).to(logits_flat.device)\n",
    "\n",
    "#             selected_logits = logits_flat[arry[mask], labels_flat[mask]]\n",
    "#             output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "#             output_logits[mask] = selected_logits\n",
    "\n",
    "#             output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "#             out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "#         else:\n",
    "#             out.gpe_score = None\n",
    "        loss = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass]\n",
    "            rank_score = out.rank_score\n",
    "#             gen_score = out.gpe_score\n",
    "\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "#             loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct1(rank_score, pass_label.view(-1))\n",
    "#             gen_loss = loss_fct2(gen_score, pass_label.view(-1))\n",
    "\n",
    "#             loss = rank_loss + gen_loss\n",
    "            \n",
    "            loss = rank_loss\n",
    "\n",
    "        ret =  Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=out.logits,\n",
    "            past_key_values=out.past_key_values,\n",
    "            decoder_hidden_states=out.decoder_hidden_states,\n",
    "            decoder_attentions=out.decoder_attentions,\n",
    "            cross_attentions=out.cross_attentions,\n",
    "            encoder_last_hidden_state=out.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=out.encoder_hidden_states,\n",
    "            encoder_attentions=out.encoder_attentions,\n",
    "        )\n",
    "        ret.rank_score = out.rank_score\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d862bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "class RankT5GPE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "#         config.rank_score_index = 32019\n",
    "#         config.n_pass = 7\n",
    "#         config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "        self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "#             print(decoder_input_ids.size())\n",
    "\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "        rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :])\n",
    "#         try:\n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass)\n",
    "#         except RuntimeError:\n",
    "#             print(rank_score.size())\n",
    "#             assert 2 == 3\n",
    "            \n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels != None:\n",
    "            logits = out.logits\n",
    "            batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = (labels_flat != -100)\n",
    "            arry = torch.arange(batch_size_n * sequence_length).to(logits_flat.device)\n",
    "\n",
    "            selected_logits = logits_flat[arry[mask], labels_flat[mask]]\n",
    "            output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "            output_logits[mask] = selected_logits\n",
    "\n",
    "            output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "            out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "        else:\n",
    "            out.gpe_score = None\n",
    "            \n",
    "#         out.rank_loss = None\n",
    "#         out.kl_d_loss = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass]\n",
    "            rank_score = F.log_softmax(out.rank_score/float(config.temp2), dim=-1) # S\n",
    "            gen_score = F.log_softmax((out.gpe_score/float(config.temp1)), dim=-1) # T\n",
    "            # K(rs||g) = g.log(rs/g)\n",
    "\n",
    "            loss_fct1 = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "            loss_fct3 = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct2(out.rank_score, pass_label.view(-1))\n",
    "            kl_d_loss = loss_fct1(rank_score, gen_score.detach()) + loss_fct3(gen_score, rank_score.detach())\n",
    "            # kl = softmax(gen_score).log(softmax(rank_score)/softmax(gen_score))\n",
    "            \n",
    "#             out.rank_loss = rank_loss\n",
    "#             out.kl_d_loss = kl_d_loss\n",
    "            loss = rank_loss + config.lamda*kl_d_loss\n",
    "\n",
    "        ret =  Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=out.logits,\n",
    "            past_key_values=out.past_key_values,\n",
    "            decoder_hidden_states=out.decoder_hidden_states,\n",
    "            decoder_attentions=out.decoder_attentions,\n",
    "            cross_attentions=out.cross_attentions,\n",
    "            encoder_last_hidden_state=out.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=out.encoder_hidden_states,\n",
    "            encoder_attentions=out.encoder_attentions,\n",
    "        )\n",
    "        ret.rank_score = out.rank_score\n",
    "        return ret\n",
    "    \n",
    "       \n",
    "# mod_ckp = \"t5-base\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(mod_ckp)\n",
    "# question = \"question:\"\n",
    "# title = \"title:\"\n",
    "# context = \"context:\"\n",
    "# eou = \"<eou>\"\n",
    "# tokenizer.add_tokens([question, title, context, eou], special_tokens=True)\n",
    "# config = T5Config.from_pretrained(mod_ckp)\n",
    "# config.output_hidden_states = True\n",
    "# config.rank_score_index = tokenizer.convert_tokens_to_ids(\"<extra_id_80>\")\n",
    "# config.n_pass = 7\n",
    "# config.temp1 = 1\n",
    "# config.temp2 = 1\n",
    "# config.lamda = 0.5\n",
    "# model = RankT5GPE.from_pretrained(mod_ckp, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752fed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "class RankT5GPE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "#         config.rank_score_index = 32019\n",
    "#         config.n_pass = 7\n",
    "#         config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "        self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "#             print(decoder_input_ids.size())\n",
    "\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "        rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :])\n",
    "#         try:\n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass)\n",
    "#         except RuntimeError:\n",
    "#             print(rank_score.size())\n",
    "#             assert 2 == 3\n",
    "            \n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels != None:\n",
    "            logits = out.logits\n",
    "            batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = (labels_flat != -100)\n",
    "            arry = torch.arange(batch_size_n * sequence_length).to(logits_flat.device)\n",
    "\n",
    "            selected_logits = logits_flat[arry[mask], labels_flat[mask]]\n",
    "            output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "            output_logits[mask] = selected_logits\n",
    "\n",
    "            output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "            out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "        else:\n",
    "            out.gpe_score = None\n",
    "            \n",
    "#         out.rank_loss = None\n",
    "#         out.kl_d_loss = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass]\n",
    "            rank_score = F.log_softmax(out.rank_score/float(config.temp2), dim=-1) # S\n",
    "            gen_score = F.softmax((out.gpe_score/float(config.temp1)), dim=-1) # T\n",
    "            # K(rs||g) = g.log(rs/g)\n",
    "\n",
    "            loss_fct1 = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct2(out.rank_score, pass_label.view(-1))\n",
    "            kl_d_loss = loss_fct1(rank_score, gen_score)\n",
    "            # kl = softmax(gen_score).log(softmax(rank_score)/softmax(gen_score))\n",
    "            \n",
    "#             out.rank_loss = rank_loss\n",
    "#             out.kl_d_loss = kl_d_loss\n",
    "            loss = rank_loss + kl_d_loss\n",
    "\n",
    "        ret =  Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=out.logits,\n",
    "            past_key_values=out.past_key_values,\n",
    "            decoder_hidden_states=out.decoder_hidden_states,\n",
    "            decoder_attentions=out.decoder_attentions,\n",
    "            cross_attentions=out.cross_attentions,\n",
    "            encoder_last_hidden_state=out.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=out.encoder_hidden_states,\n",
    "            encoder_attentions=out.encoder_attentions,\n",
    "        )\n",
    "        ret.rank_score = out.rank_score\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b322681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RankT5GPE.from_pretrained(model_ckpt)\n",
    "# model = RankT5.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ad32aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.retrieval import RetrievalMRR, RetrievalRecall, RetrievalNormalizedDCG\n",
    "def cal_mrr(rank_score, pass_label):\n",
    "    pass_label = pass_label.view(-1)[::7]\n",
    "    target = torch.nn.functional.one_hot(pass_label, 7)\n",
    "    indexes = torch.arange(rank_score.size()[0]).view(-1, 1).expand(rank_score.size()[0], rank_score.size()[1])\n",
    "    mr = RetrievalMRR()\n",
    "    return mr(rank_score, target, indexes=indexes)\n",
    "\n",
    "def r_at_k(rank_score, pass_label, k):\n",
    "    pass_label = pass_label.view(-1)[::7]\n",
    "    target = torch.nn.functional.one_hot(pass_label, 7).to(pass_label.device)\n",
    "    indexes = torch.arange(rank_score.size()[0]).view(-1, 1).expand(rank_score.size()[0], rank_score.size()[1]).to(pass_label.device)\n",
    "#     print(rank_score.shape)\n",
    "    rk = RetrievalRecall(top_k=k)\n",
    "    return rk(rank_score, target, indexes=indexes)\n",
    "\n",
    "def ndgc_at_k(rank_score, pass_label, k):\n",
    "    pass_label = pass_label.view(-1)[::7]\n",
    "    target = torch.nn.functional.one_hot(pass_label, 7).to(pass_label.device)\n",
    "    indexes = torch.arange(rank_score.size()[0]).view(-1, 1).expand(rank_score.size()[0], rank_score.size()[1]).to(pass_label.device)\n",
    "    rk = RetrievalNormalizedDCG(top_k=k)\n",
    "    return rk(rank_score, target, indexes=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f0b677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2875e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "dl = DataLoader(test, batch_size=56, collate_fn=data_collator, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd64bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d42b98f7617493e8f7c1c919087d863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from tqdm.notebook import tqdm\n",
    "model = model.to(device)\n",
    "pred = None\n",
    "lab_p = None\n",
    "batch_len = []\n",
    "mrr = []\n",
    "r_k = {}\n",
    "for i in range(1, 8):\n",
    "    r_k[i] = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(dl):\n",
    "        batch_len.append(int(b[\"input_ids\"].size()[0]))\n",
    "        input_ids = b[\"input_ids\"].to(device)\n",
    "        attention_mask = b[\"attention_mask\"].to(device)\n",
    "        decoder_input_ids = b[\"decoder_input_ids\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        rank_score = outputs.rank_score\n",
    "        pass_label = b[\"pass_label\"].to(device)\n",
    "        \n",
    "#         print(pass_label.size())\n",
    "        mrr.append(cal_mrr(rank_score, pass_label))\n",
    "        if pred == None:\n",
    "            pred = rank_score\n",
    "            lab_p = pass_label\n",
    "        else:\n",
    "            pred = torch.cat((pred, rank_score), 0)\n",
    "            lab_p = torch.cat((lab_p, pass_label), 0)\n",
    "\n",
    "        for i in r_k.keys():\n",
    "            r_k[i].append(r_at_k(rank_score, pass_label, i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c7ed4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank: 0.8529314398765564\n",
      "Recall@1: 0.7606093287467957\n",
      "Recall@2: 0.8846572041511536\n",
      "Recall@3: 0.9330794215202332\n",
      "Recall@4: 0.962459146976471\n",
      "Recall@5: 0.9834058284759521\n",
      "Recall@6: 0.9940152168273926\n",
      "Recall@7: 1.0\n",
      "NDGC@1: 0.7606093287467957\n",
      "NDGC@2: 0.838874876499176\n",
      "NDGC@3: 0.8630859851837158\n",
      "NDGC@4: 0.8757391571998596\n",
      "NDGC@5: 0.883842408657074\n",
      "NDGC@6: 0.8876215815544128\n",
      "NDGC@7: 0.8896164894104004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Reciprocal Rank: {cal_mrr(pred, lab_p)}\")\n",
    "\n",
    "for i in range(1, 8):\n",
    "    print(f\"Recall@{i}: {r_at_k(pred, lab_p, i)}\")\n",
    "\n",
    "for i in range(1, 8):\n",
    "    print(f\"NDGC@{i}: {ndgc_at_k(pred, lab_p, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_wow[155][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e930bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DatasetDict.load_from_disk(\"wow_rank_ut_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4514a71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: Ah my favorite kind of beer are the stouts. Like a nice milk or chocolate stout with a much larger alcohol content. title: Beer passage: Beer is one of the oldest and most widely consumed alcoholic drinks in the world, and the third most popular drink overall after water and tea. Beer is brewed from cereal grainsâ€”most commonly from malted barley, though wheat, maize (corn), and rice are also used. During the brewing process, fermentation of the starch sugars in the wort produces ethanol and carbonation in the resulting beer. Most modern beer is brewed with hops, which add bitterness and other flavours and act as a natural preservative and stabilizing agent. Other flavouring agents such as gruit, herbs, or fruits may be included or used instead of hops. In commercial brewing, the natural carbonation effect is often removed during processing and replaced with forced carbonation. Some of humanity\\'s earliest known writings refer to the production and distribution of beer: the Code of Hammurabi included laws regulating beer and beer parlours, and \"The Hymn to Ninkasi\", a prayer to the Mesopotamian goddess of beer, served as both a prayer and as a method of remembering the recipe for beer in a culture with few literate people. Beer is distributed in bottles and cans and is also commonly available on draught, particularly in pubs and bars.</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dt[\"test\"][155][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbl = sum(batch_len)/7\n",
    "batch_len = [a/7 for a in batch_len]\n",
    "# mrr_score =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8811b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = [(a*b)/sbl for a, b in zip(mrr, batch_len)]\n",
    "print(f\"Mean Reciprocal Rank: {sum(mrr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in r_k.keys():\n",
    "    print(f\"Recall@{k}: {sum([(a*b)/sbl for a, b in zip(r_k[k], batch_len)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed180d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(r_k[3])/len(r_k[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randint(1, 34, (20, 5))\n",
    "p = torch.randint(1, 34, (20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be85672",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27017522",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((t, p), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5625e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = torch.arange(x.rank_score.size()[0]).view(-1, 1).expand(x.rank_score.size()[0], x.rank_score.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18627da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4643125",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.rank_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66575c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.retrieval import RetrievalMRR\n",
    "mr = RetrievalMRR()\n",
    "met = mr(x.rank_score, t, indexes=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2428*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6856c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 in torch.tensor([1, 4, 5, 2, 0, 3, 6][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(x.logits.view(7, -1)[:, 23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.decoder_hidden_states[-1][:, 0, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros((70, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a list of rank tensors (output from previous code)\n",
    "rank_tensors = [\n",
    "    torch.tensor([3, 1, 4, 2]),\n",
    "    torch.tensor([2, 3, 1, 4]),\n",
    "    # Add more rank tensors here\n",
    "]\n",
    "\n",
    "# Calculate MRR\n",
    "mrr_sum = 0.0\n",
    "total_queries = len(rank_tensors)\n",
    "\n",
    "for ranks in rank_tensors:\n",
    "    reciprocal_ranks = 1.0 / ranks.float()\n",
    "    mrr_sum += reciprocal_ranks.sum()\n",
    "    print(mrr_sum)\n",
    "\n",
    "mrr = mrr_sum / total_queries\n",
    "\n",
    "print(\"MRR:\", mrr.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167a423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
