{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9d59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  8 21:01:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    58W / 300W |  31022MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   47C    P0   259W / 300W |   7984MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   54C    P0   279W / 300W |  19863MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    43W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    44W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   48C    P0   278W / 300W |   7890MiB / 32510MiB |     79%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    45W / 300W |      7MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   142W / 300W |   3950MiB / 32510MiB |     85%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1370995      C   .../envs/work_env/bin/python    31017MiB |\n",
      "|    1   N/A  N/A   1766696      C   python3                          7981MiB |\n",
      "|    2   N/A  N/A   1598486      C   python3                          2683MiB |\n",
      "|    2   N/A  N/A   3281871      C   python                          17175MiB |\n",
      "|    5   N/A  N/A   1766696      C   python3                          7887MiB |\n",
      "|    7   N/A  N/A   3328743      C   python                           3947MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e09c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47328b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tokenized_wow = DatasetDict.load_from_disk(\"wow_rank_kel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d065433",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wow.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"pass_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790ebc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RankT5GPE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config: T5Config):\n",
    "        config.rank_score_index = 32019\n",
    "        config.n_pass = 7\n",
    "        config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "        self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "\n",
    "        # input_ids = input_ids.view(batch_size*n_pass, -1)\n",
    "        # attention_mask = attention_mask.view(batch_size*n_pass, -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labels != None and decoder_input_ids == None:\n",
    "#             batch_size, decoder_seq_len = labels.size()\n",
    "#             labels = labels.view(batch_size, 1, decoder_seq_len).contiguous()\n",
    "#             labels = labels.expand(batch_size, n_pass, decoder_seq_len).contiguous()\n",
    "\n",
    "#             labels = labels.view(batch_size*n_pass, -1)\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "#             print(decoder_input_ids.size())\n",
    "\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, **kwargs)\n",
    "        rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :])\n",
    "        out.rank_score = rank_score[:, self.rank_id].view(-1, self.n_pass)\n",
    "\n",
    "\n",
    "        if labels != None:\n",
    "            logits = out.logits\n",
    "            batch_size_n, sequence_length, vocab_size = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(batch_size_n * sequence_length, vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = (labels_flat != -100)\n",
    "            arry = torch.arange(batch_size_n * sequence_length).to(logits_flat.device)\n",
    "\n",
    "            selected_logits = logits_flat[arry[mask], labels_flat[mask]]\n",
    "            output_logits = torch.full((batch_size_n * sequence_length,), 0, dtype=logits.dtype, device=logits.device)\n",
    "            output_logits[mask] = selected_logits\n",
    "\n",
    "            output_logits = output_logits.view(batch_size_n, -1).sum(-1)\n",
    "            out.gpe_score = output_logits.view(int(batch_size_n/self.n_pass), self.n_pass)\n",
    "\n",
    "        else:\n",
    "            out.gpe_score = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            print(pass_label)\n",
    "#             assert 21\n",
    "            pass_label = pass_label[::self.n_pass]\n",
    "            rank_score = out.rank_score\n",
    "            gen_score = out.gpe_score\n",
    "\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct1(rank_score, pass_label.view(-1))\n",
    "            gen_loss = loss_fct2(gen_score, pass_label.view(-1))\n",
    "\n",
    "            loss = rank_loss + gen_loss\n",
    "            out.loss = loss\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7d9e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of RankT5GPE were not initialized from the model checkpoint at t5-small and are newly initialized: ['rank_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "mod_ckp = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(mod_ckp)\n",
    "config = T5Config.from_pretrained(mod_ckp)\n",
    "model = RankT5GPE(config).from_pretrained(mod_ckp)\n",
    "model.config.output_hidden_states = True\n",
    "model.config.rank_score_index = tokenizer.convert_tokens_to_ids(\"<extra_id_80>\")\n",
    "model.config.n_pass = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cacd81db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32019"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<extra_id_80>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07463dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38132708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "batch_size = 8*model.config.n_pass \n",
    "model_dir = f\"wow_rank_{mod_ckp}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e62aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import datasets\n",
    "from typing import Optional\n",
    "from transformers.trainer import is_datasets_available, seed_worker\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "\n",
    "\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         print(inputs.get(\"input_ids\").size())\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
    "        training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "#             dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, shuffle=False, **dataloader_params))\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "\n",
    "        Args:\n",
    "            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n",
    "                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n",
    "                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self.args.eval_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        }\n",
    "\n",
    "        if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "#             dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(eval_dataset, shuffle=False, **dataloader_params))\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_wow[\"train\"],\n",
    "    eval_dataset=tokenized_wow[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90aadbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5af064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 0, 4, 2, 5, 0, 0, 3, 5, 3, 5, 2, 2, 0, 2, 0, 1, 4, 3, 0, 6, 0, 4,\n",
      "        1, 6, 0, 0, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 6, 4, 2,\n",
      "        0, 0, 1, 0, 4, 0, 1, 0], device='cuda:0')\n",
      "tensor([0, 2, 0, 1, 6, 0, 1, 3, 4, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1,\n",
      "        2, 6, 4, 0, 0, 0, 0, 0, 6, 3, 3, 0, 0, 3, 0, 0, 5, 0, 0, 6, 6, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
      "tensor([0, 0, 2, 0, 0, 6, 0, 3, 3, 0, 3, 0, 0, 6, 1, 6, 0, 0, 1, 2, 5, 3, 1, 1,\n",
      "        5, 2, 0, 0, 1, 3, 4, 0, 2, 4, 0, 0, 4, 0, 3, 0, 0, 4, 0, 2, 6, 1, 4, 0,\n",
      "        6, 0, 0, 4, 1, 6, 0, 0], device='cuda:0')\n",
      "tensor([3, 2, 1, 3, 2, 1, 1, 2, 0, 6, 5, 1, 6, 5, 0, 6, 2, 0, 0, 2, 4, 0, 0, 5,\n",
      "        4, 0, 0, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 1, 3, 0, 1, 0, 0, 1, 3, 3, 4,\n",
      "        3, 0, 3, 2, 1, 1, 4, 4], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2193' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/2193 00:03 < 1:10:28, 0.52 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 6, 0, 0, 0, 0, 5, 0, 0, 0, 2, 2, 1, 0, 0, 3, 0, 0, 0, 0, 0, 2, 3,\n",
      "        3, 0, 2, 1, 0, 3, 4, 6, 0, 4, 0, 4, 2, 4, 1, 0, 1, 1, 1, 3, 0, 0, 3, 0,\n",
      "        5, 0, 1, 4, 0, 0, 1, 1], device='cuda:0')\n",
      "tensor([6, 6, 0, 2, 0, 0, 5, 0, 5, 0, 0, 0, 2, 0, 1, 1, 3, 6, 0, 0, 1, 0, 5, 1,\n",
      "        0, 5, 0, 1, 2, 0, 3, 3, 1, 0, 2, 2, 1, 1, 0, 2, 0, 4, 1, 0, 2, 6, 5, 0,\n",
      "        3, 0, 4, 6, 3, 0, 4, 5], device='cuda:0')\n",
      "tensor([4, 1, 0, 1, 4, 1, 1, 2, 6, 0, 1, 3, 4, 2, 6, 0, 0, 1, 0, 6, 2, 1, 0, 1,\n",
      "        3, 0, 0, 3, 2, 0, 1, 6, 0, 0, 6, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 6, 5, 1,\n",
      "        0, 3, 0, 0, 4, 1, 1, 0], device='cuda:0')\n",
      "tensor([0, 0, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 3, 4, 2, 0, 5, 3, 0, 3, 1, 5, 0, 3,\n",
      "        5, 0, 6, 3, 0, 0, 6, 5, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 3, 4, 2, 1, 3,\n",
      "        0, 4, 0, 0, 2, 2, 2, 2], device='cuda:0')\n",
      "tensor([0, 3, 0, 1, 0, 3, 4, 0, 0, 0, 5, 4, 2, 0, 5, 0, 2, 0, 0, 2, 1, 0, 0, 0,\n",
      "        1, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 4, 1, 6, 0, 5, 1, 1, 0, 0, 1, 1, 6, 0,\n",
      "        1, 1, 5, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([2, 2, 1, 1, 1, 2, 0, 3, 0, 0, 0, 4, 0, 6, 2, 0, 5, 6, 0, 4, 0, 1, 5, 3,\n",
      "        2, 2, 0, 5, 0, 0, 2, 0, 2, 6, 0, 1, 0, 1, 0, 5, 5, 0, 0, 1, 5, 2, 1, 0,\n",
      "        0, 0, 6, 3, 1, 2, 2, 0], device='cuda:0')\n",
      "tensor([0, 1, 1, 1, 0, 0, 6, 0, 1, 0, 6, 0, 3, 1, 0, 0, 3, 6, 0, 0, 1, 1, 0, 0,\n",
      "        2, 1, 5, 1, 0, 0, 0, 0, 5, 0, 2, 3, 0, 4, 0, 0, 3, 0, 1, 4, 0, 0, 2, 0,\n",
      "        0, 0, 0, 5, 1, 1, 6, 0], device='cuda:0')\n",
      "tensor([0, 4, 0, 0, 0, 6, 5, 5, 3, 2, 0, 0, 6, 6, 1, 2, 2, 1, 0, 1, 4, 0, 2, 0,\n",
      "        0, 0, 4, 4, 0, 6, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 3, 2, 3, 3, 4, 4, 1, 2,\n",
      "        1, 1, 1, 2, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor([5, 6, 6, 3, 0, 0, 0, 5, 0, 1, 4, 0, 0, 0, 0, 1, 4, 1, 0, 5, 1, 0, 0, 0,\n",
      "        2, 0, 0, 5, 0, 5, 2, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 3, 1, 0, 3, 3, 4,\n",
      "        0, 2, 1, 2, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([5, 0, 1, 5, 0, 1, 1, 6, 1, 1, 1, 0, 1, 1, 2, 1, 5, 0, 5, 2, 0, 4, 1, 0,\n",
      "        0, 0, 0, 5, 0, 0, 4, 0, 0, 0, 6, 1, 0, 0, 0, 1, 3, 0, 6, 1, 1, 2, 0, 2,\n",
      "        2, 0, 5, 1, 1, 0, 1, 2], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         print(inputs.get(\"input_ids\").size())\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mRankT5GPE.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, labels, pass_label, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m labels_flat \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m mask \u001b[38;5;241m=\u001b[39m (labels_flat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m arry \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m selected_logits \u001b[38;5;241m=\u001b[39m logits_flat[arry[mask], labels_flat[mask]]\n\u001b[1;32m     50\u001b[0m output_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size_n \u001b[38;5;241m*\u001b[39m sequence_length,), \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d679c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1691478743.dgx1-2\r\n"
     ]
    }
   ],
   "source": [
    "!ls wow_rank_t5-small/runs/Aug08_12-42-17_dgx1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30c0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"wow_kle_t5_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c17a741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\tspecial_tokens_map.json  training_args.bin\r\n",
      "generation_config.json\tspiece.model\r\n",
      "pytorch_model.bin\ttokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls wow_kle_t5_base/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6551a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
